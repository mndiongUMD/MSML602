{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386c985f",
   "metadata": {},
   "source": [
    "In this project, we are predicting trends in technology adoption and interest based on social media (Twitter) data. Specifically, the model aims to forecast the following:\n",
    "\n",
    "1. **Volume of Discussions**: Predicting the number of tweets or social media posts related to specific technologies, gadgets, or software within a given time frame in the future (e.g., daily, weekly). This serves as an indicator of public interest and awareness levels.\n",
    "\n",
    "2. **Sentiment Trends**: Forecasting the overall sentiment (positive, negative, neutral) associated with these technologies in the social media discourse. This could involve predicting the average sentiment score or the proportion of tweets falling into each sentiment category for upcoming days.\n",
    "\n",
    "3. **Combination of Volume and Sentiment**: A more comprehensive approach might involve predicting both the volume of discussion and the sentiment concurrently. This dual prediction can provide a more nuanced understanding of how public interest and perception might evolve over time.\n",
    "\n",
    "### Example Predictions\n",
    "- **Before a Product Launch**: If there's an upcoming release of a new gadget, the model might predict an increase in the volume of discussion and potentially the sentiment trend leading up to and following the launch.\n",
    "- **Emerging Technology Trends**: For emerging tech like augmented reality, blockchain, or new software platforms, the model could forecast how discussions (both in volume and sentiment) about these technologies will trend in the short-term future.\n",
    "\n",
    "### Purpose of These Predictions\n",
    "- **Market Insight**: These predictions can provide valuable insights for businesses, marketers, and technologists about consumer interest and sentiment trends, aiding in strategic planning and decision-making.\n",
    "- **Product Strategy**: For tech companies, understanding how public interest and sentiment are likely to shift can inform product development, marketing strategies, and customer engagement plans.\n",
    "- **Investment Decisions**: Investors in technology sectors might use these predictions to gauge potential market reactions to new technologies or products.\n",
    "\n",
    "The predictions, therefore, are not just about the raw data but also about interpreting the data to extract meaningful trends and insights that can inform various strategic decisions in the technology domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c50608bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For web scraping\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import tweepy\n",
    "\n",
    "# Data Storage\n",
    "import sqlite3\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# For advanced data manipulation\n",
    "from scipy import stats\n",
    "\n",
    "# For machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# For working with APIs\n",
    "import json\n",
    "\n",
    "# For datetime operations\n",
    "from datetime import datetime\n",
    "\n",
    "# Additional utilities\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "# For time series analysis\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# For real-time data streaming\n",
    "import websocket\n",
    "import socket\n",
    "\n",
    "# For asynchronous programming (useful for real-time data processing)\n",
    "import asyncio\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23da919",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "## 2.1 Twitter API Access\n",
    "Apply for Access: If you haven't already, apply for a Twitter Developer account at developer.twitter.com. You'll need to explain your project's purpose and how you'll use the data.\n",
    "Create an Application: Once approved, create a new application in the Twitter Developer portal to get your API keys and tokens — these are necessary for accessing the Twitter API.\n",
    "\n",
    "## 2.2 Understand Twitter API Limitations\n",
    "Rate Limits: Familiarize yourself with the Twitter API rate limits to avoid hitting the cap on the number of requests.\n",
    "Data Availability: Twitter API provides access to tweets from the last 7 days for the standard search API, and more historical data with the premium or enterprise tiers.\n",
    "\n",
    "## 2.3 Develop Data Collection Script\n",
    "Install Tweepy: Use the Python library tweepy for interacting with the Twitter API. Install it via pip (pip install tweepy).\n",
    "Authentication: Use your API keys and tokens to authenticate your requests.\n",
    "Querying Tweets: Write a function to query tweets based on the product or brand name inputted by the user. Use query parameters effectively to filter and retrieve relevant tweets.\n",
    "Handling Rate Limiting: Implement logic to handle rate limiting by the Twitter API, such as waiting and retrying after a certain period.\n",
    "\n",
    "## 2.4 Store Collected Data\n",
    "Temporary Storage: Initially, you might store the tweets in a temporary data structure like a list or a Pandas DataFrame.\n",
    "Database Storage: For long-term storage and retrieval, consider saving the tweets to a database. Choose between SQL or NoSQL based on your preference and the data's structure.\n",
    "\n",
    "## 2.5 Error Handling\n",
    "API Errors: Implement error handling for issues like network errors, API rate limits, or invalid responses.\n",
    "Data Quality Checks: Put checks in place to ensure the quality of the data collected (e.g., filtering out irrelevant or spammy tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f7893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API keys\n",
    "api_key = 'F0qKO41dErn04DpsRuAtnnSaT'\n",
    "api_secret_key = 'TKTT695N6shmAVxsSVzVRGUF9CSKLqoIPrkeHLDHHfj5UaNHUv'\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAEPSjQEAAAAA3DQLgk5ybCdfGUtqI%2FKv4SruAHY%3DhPf2TpZzegGcm4L3ExHFmATJmXl5VECRIHJPxhfZwxpuYTsf4U'\n",
    "access_token = '2931998159-oEfo3wO1SsEkil6NJ1T3Wni7lvdciTKLIvNeUz3'\n",
    "access_token_secret = 'Pu7kueCRteEwU28vzqpsCh0Y0AQ9y0wIqW8VssrZUoDDN'\n",
    "client_id = 'cl9xZUpDZE9Bb01aZUdIWWQ3aFM6MTpjaQ'\n",
    "client_id_secret = 'qHcKpBGB1YLgIQdRfcgMf4YCBzZpYy_OQlkf67mE_afJ1T2C3l'\n",
    "\n",
    "# Authenticate\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07da0d70",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "API.__init__() got an unexpected keyword argument 'wait_on_rate_limit_notify'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m auth\u001b[38;5;241m.\u001b[39mset_access_token(access_token, access_token_secret)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create API object\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m api \u001b[38;5;241m=\u001b[39m tweepy\u001b[38;5;241m.\u001b[39mAPI(auth, wait_on_rate_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m                  wait_on_rate_limit_notify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Define a class to listen to tweets\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyStreamListener\u001b[39;00m(tweepy\u001b[38;5;241m.\u001b[39mStreamListener):\n",
      "\u001b[1;31mTypeError\u001b[0m: API.__init__() got an unexpected keyword argument 'wait_on_rate_limit_notify'"
     ]
    }
   ],
   "source": [
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Create API object\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "                 wait_on_rate_limit_notify=True)\n",
    "\n",
    "# Define a class to listen to tweets\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        # Extract info from tweets\n",
    "        if status.retweeted:\n",
    "            return\n",
    "        text = status.text\n",
    "        sentiment_score = analyze_sentiment(text)\n",
    "        # Process and store the tweet and sentiment score\n",
    "\n",
    "# Instantiate the stream listener\n",
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n",
    "\n",
    "# Start the stream\n",
    "# Use Tweepy to stream tweets containing keywords like \n",
    "# \"artificial intelligence\", \"augmented reality\", \"blockchain\".\n",
    "myStream.filter(track=['artificial intelligence', 'augmented reality', 'blockchain'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "766a72a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\n453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m product_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGPT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m max_tweets \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m---> 12\u001b[0m tweets_about_product \u001b[38;5;241m=\u001b[39m search_tweets(product_name, max_tweets)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Print the fetched tweets\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets_about_product:\n",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m, in \u001b[0;36msearch_tweets\u001b[1;34m(query, max_tweets)\u001b[0m\n\u001b[0;32m      2\u001b[0m tweets \u001b[38;5;241m=\u001b[39m tweepy\u001b[38;5;241m.\u001b[39mCursor(api\u001b[38;5;241m.\u001b[39msearch_tweets, q\u001b[38;5;241m=\u001b[39mquery, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, tweet_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextended\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mitems(max_tweets)\n\u001b[0;32m      4\u001b[0m tweet_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets:\n\u001b[0;32m      6\u001b[0m     tweet_list\u001b[38;5;241m.\u001b[39mappend(tweet\u001b[38;5;241m.\u001b[39mfull_text)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tweet_list\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\cursor.py:86\u001b[0m, in \u001b[0;36mBaseIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\cursor.py:286\u001b[0m, in \u001b[0;36mItemIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_page \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_page) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Reached end of current page, get the next page...\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_iterator)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_page) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_iterator)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\cursor.py:86\u001b[0m, in \u001b[0;36mBaseIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\cursor.py:167\u001b[0m, in \u001b[0;36mIdIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 167\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod(max_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_id, parser\u001b[38;5;241m=\u001b[39mRawParser(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m     model \u001b[38;5;241m=\u001b[39m ModelParser()\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m    170\u001b[0m         data, api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m,\n\u001b[0;32m    171\u001b[0m         payload_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mpayload_list,\n\u001b[0;32m    172\u001b[0m         payload_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mpayload_type\n\u001b[0;32m    173\u001b[0m     )\n\u001b[0;32m    174\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m    175\u001b[0m         data, api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m,\n\u001b[0;32m    176\u001b[0m         payload_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mpayload_list,\n\u001b[0;32m    177\u001b[0m         payload_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mpayload_type\n\u001b[0;32m    178\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\api.py:33\u001b[0m, in \u001b[0;36mpagination.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\api.py:46\u001b[0m, in \u001b[0;36mpayload.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_list\n\u001b[0;32m     45\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_type\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\api.py:1146\u001b[0m, in \u001b[0;36mAPI.search_tweets\u001b[1;34m(self, q, **kwargs)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;129m@pagination\u001b[39m(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;129m@payload\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_results\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_tweets\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"search_tweets(q, *, geocode, lang, locale, result_type, count, \\\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;124;03m                     until, since_id, max_id, include_entities)\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    .. _Twitter's documentation on the standard search API: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m   1147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch/tweets\u001b[39m\u001b[38;5;124m'\u001b[39m, endpoint_parameters\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeocode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1149\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muntil\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msince_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_entities\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1150\u001b[0m         ), q\u001b[38;5;241m=\u001b[39mq, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1151\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\api.py:271\u001b[0m, in \u001b[0;36mAPI.request\u001b[1;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(resp)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(resp)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFound(resp)\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 Forbidden\n453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product"
     ]
    }
   ],
   "source": [
    "def search_tweets(query, max_tweets):\n",
    "    tweets = tweepy.Cursor(api.search_tweets, q=query, lang=\"en\", tweet_mode='extended').items(max_tweets)\n",
    "    \n",
    "    tweet_list = []\n",
    "    for tweet in tweets:\n",
    "        tweet_list.append(tweet.full_text)\n",
    "\n",
    "    return tweet_list\n",
    "\n",
    "product_name = \"ChatGPT\"\n",
    "max_tweets = 50\n",
    "tweets_about_product = search_tweets(product_name, max_tweets)\n",
    "\n",
    "# Print the fetched tweets\n",
    "for tweet in tweets_about_product:\n",
    "    print(tweet)\n",
    "    store_tweet(tweet, product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00debcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the tweets in a database\n",
    "\n",
    "def create_database():\n",
    "    # Connect to SQLite database (it will be created if it doesn't exist)\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "\n",
    "    # Create a new SQLite table with columns for different tweet attributes\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS tweets\n",
    "                 (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                  tweet_text TEXT,\n",
    "                  query TEXT,\n",
    "                  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    \n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "create_database()\n",
    "\n",
    "def store_tweet(tweet_text, query):\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Insert a new row of data\n",
    "    cur.execute(\"INSERT INTO tweets (tweet_text, query) VALUES (?, ?)\", (tweet_text, query))\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "# For fetching the data in the database later\n",
    "def get_tweets_by_query(query):\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Select tweets that match the query\n",
    "    cur.execute(\"SELECT tweet_text FROM tweets WHERE query=?\", (query,))\n",
    "    all_tweets = cur.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "    return all_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb8c7b",
   "metadata": {},
   "source": [
    "# Step 3: Data Processing\n",
    "3.1 Clean and Preprocess Data\n",
    "Implement functions to clean tweets (removing URLs, mentions, hashtags, and special characters).\n",
    "Normalize text data (like converting to lowercase, removing punctuation).\n",
    "\n",
    "Remove URLs: URLs in tweets can be removed as they usually don't contribute to sentiment analysis.\n",
    "Remove Mentions and Hashtags: Mentions (@usernames) and hashtags (#hashtag) can also be removed or kept based on your analysis requirement.\n",
    "Remove Special Characters and Numbers: Special characters and numbers often don't contribute to sentiment analysis and can be removed.\n",
    "Convert to Lowercase: Convert all texts to lowercase to maintain consistency.\n",
    "\n",
    "3.2 Data Storage\n",
    "Decide on how you'll store the fetched tweets (e.g., in a database or files).\n",
    "Implement the storage mechanism in your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201eceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)  # Remove URLs\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)  # Remove mentions\n",
    "    tweet = re.sub(r'#\\S+', '', tweet)  # Remove hashtags\n",
    "    tweet = re.sub(r'[^A-Za-z\\s]', '', tweet)  # Remove special characters and numbers\n",
    "    tweet = tweet.lower()  # Convert to lowercase\n",
    "    return tweet\n",
    "\n",
    "cleaned_tweets = clean_tweet(tweets_about_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6840c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tweet(tweet):\n",
    "    words = tweet.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    words = tweet.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = clean_tweet(tweet)\n",
    "    tweet = lemmatize_tweet(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc92c7",
   "metadata": {},
   "source": [
    "# Step 4: Data Aggregation for Time Series\n",
    "**Aggregate Data:** Aggregate the data by your chosen time intervals (e.g., daily). You’ll want to sum up the number of tweets and calculate average sentiment scores for each interval.\n",
    "\n",
    "**Exploratory Data Analysis Trend Analysis:** Visualize the volume of tweets and average sentiment over time to identify patterns, trends, and anomalies. Correlation Analysis: Optionally, check if there's any correlation between the volume of tweets or sentiment with external events or announcements in the tech world.\n",
    "\n",
    "**Time-Series Forecasting Model Selection:** Choose a suitable model for time-series forecasting. ARIMA, SARIMA, and LSTM neural networks are common choices. Feature Engineering: Include relevant time-based features and any other features that might improve the model. Model Training: Train your model on the historical data.\n",
    "\n",
    "**Model Evaluation Validation:** Validate your model on a separate test set. Performance Metrics: Use metrics like MAE, RMSE, or others relevant for time-series to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2cc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(tweets_about_product)\n",
    "\n",
    "# Convert timestamp to datetime and set as index\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Resample and aggregate\n",
    "aggregated_df = df.resample('D').agg({'text': 'count', 'sentiment': 'mean'}) # 'D' for daily\n",
    "aggregated_df.rename(columns={'text': 'tweet_count'}, inplace=True)\n",
    "\n",
    "# Now, aggregated_df contains daily tweet counts and average sentiment\n",
    "print(aggregated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3423d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values with the previous day's data (forward fill)\n",
    "# It contains columns like 'tweet_count' and 'average_sentiment'\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values before forward fill:\")\n",
    "print(aggregated_df.isnull().sum())\n",
    "\n",
    "# Apply forward fill\n",
    "aggregated_df.ffill(inplace=True)\n",
    "\n",
    "# Check to ensure missing values are filled\n",
    "print(\"Missing values after forward fill:\")\n",
    "print(aggregated_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a29a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ARIMA (Example)\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Assuming df is your DataFrame and 'tweet_count' is the column\n",
    "# Check for stationarity\n",
    "result = adfuller(df['tweet_count'])\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "\n",
    "# Fit ARIMA model (example parameters)\n",
    "model = ARIMA(df['tweet_count'], order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Vector Autoregression (Example)\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Assuming df is your DataFrame with multiple columns like 'tweet_count' and 'sentiment'\n",
    "model = VAR(df)\n",
    "model_fit = model.fit(maxlags=15, ic='aic')\n",
    "print(model_fit.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4380e",
   "metadata": {},
   "source": [
    "# Step 5: Exploratory Data Analysis (EDA)\n",
    "**Trend Analysis:** Visualize the volume of tweets and average sentiment over time to identify patterns, trends, and anomalies.\n",
    "\n",
    "**Correlation Analysis:** Optionally, check if there's any correlation between the volume of tweets or sentiment with external events or announcements in the tech world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966063d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'aggregated_df' has columns 'tweet_count' and 'average_sentiment'\n",
    "aggregated_df.plot(subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c797c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Augmented Dickey-Fuller test\n",
    "result = adfuller(aggregated_df['tweet_count'])\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "plot_acf(aggregated_df['tweet_count'])\n",
    "plot_pacf(aggregated_df['tweet_count'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f23dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition = seasonal_decompose(aggregated_df['tweet_count'], model='additive')\n",
    "decomposition.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(aggregated_df.corr(), annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f9c03",
   "metadata": {},
   "source": [
    "# Step 6: Time-Series Forecasting\n",
    "**Model Selection:** Choose a suitable model for time-series forecasting. ARIMA, SARIMA, and LSTM neural networks are common choices.\n",
    "\n",
    "**Feature Engineering:** Include relevant time-based features and any other features that might improve the model.\n",
    "Model Training: Train your model on the historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Example: ARIMA with parameters (1,1,1)\n",
    "model = ARIMA(train_data, order=(1,1,1))\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "model = VAR(train_data)\n",
    "model_fit = model.fit(ic='aic')  # 'aic' will choose the best lag order based on AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783490a9",
   "metadata": {},
   "source": [
    "# Step 7: Model Evaluation\n",
    "**Validation:** Validate your model on a separate test set.\n",
    "\n",
    "**Performance Metrics:** Use metrics like MAE, RMSE, or others relevant for time-series to evaluate the model's performance.# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e752e3",
   "metadata": {},
   "source": [
    "# Step 8: Prediction and Visualization\n",
    "**Make Predictions:** Use the model to make predictions about future trends in discussions on emerging technologies.\n",
    "\n",
    "**Visualization:** Create visualizations to represent the predicted trends in an understandable and insightful way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
