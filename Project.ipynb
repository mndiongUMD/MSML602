{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48067c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import tweepy\n",
    "\n",
    "# Data Storage\n",
    "import sqlite3\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# For advanced data manipulation\n",
    "from scipy import stats\n",
    "\n",
    "# For machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# For working with APIs\n",
    "import json\n",
    "\n",
    "# For datetime operations\n",
    "from datetime import datetime\n",
    "\n",
    "# Additional utilities\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "# For time series analysis\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# For real-time data streaming\n",
    "import websocket\n",
    "import socket\n",
    "\n",
    "# For asynchronous programming (useful for real-time data processing)\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dffce38",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "## 2.1 Twitter API Access\n",
    "Apply for Access: If you haven't already, apply for a Twitter Developer account at developer.twitter.com. You'll need to explain your project's purpose and how you'll use the data.\n",
    "Create an Application: Once approved, create a new application in the Twitter Developer portal to get your API keys and tokens â€” these are necessary for accessing the Twitter API.\n",
    "\n",
    "## 2.2 Understand Twitter API Limitations\n",
    "Rate Limits: Familiarize yourself with the Twitter API rate limits to avoid hitting the cap on the number of requests.\n",
    "Data Availability: Twitter API provides access to tweets from the last 7 days for the standard search API, and more historical data with the premium or enterprise tiers.\n",
    "\n",
    "## 2.3 Develop Data Collection Script\n",
    "Install Tweepy: Use the Python library tweepy for interacting with the Twitter API. Install it via pip (pip install tweepy).\n",
    "Authentication: Use your API keys and tokens to authenticate your requests.\n",
    "Querying Tweets: Write a function to query tweets based on the product or brand name inputted by the user. Use query parameters effectively to filter and retrieve relevant tweets.\n",
    "Handling Rate Limiting: Implement logic to handle rate limiting by the Twitter API, such as waiting and retrying after a certain period.\n",
    "\n",
    "## 2.4 Store Collected Data\n",
    "Temporary Storage: Initially, you might store the tweets in a temporary data structure like a list or a Pandas DataFrame.\n",
    "Database Storage: For long-term storage and retrieval, consider saving the tweets to a database. Choose between SQL or NoSQL based on your preference and the data's structure.\n",
    "\n",
    "## 2.5 Error Handling\n",
    "API Errors: Implement error handling for issues like network errors, API rate limits, or invalid responses.\n",
    "Data Quality Checks: Put checks in place to ensure the quality of the data collected (e.g., filtering out irrelevant or spammy tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c78bd2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API keys\n",
    "api_key = 'F0qKO41dErn04DpsRuAtnnSaT'\n",
    "api_secret_key = 'TKTT695N6shmAVxsSVzVRGUF9CSKLqoIPrkeHLDHHfj5UaNHUv'\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAEPSjQEAAAAA3DQLgk5ybCdfGUtqI%2FKv4SruAHY%3DhPf2TpZzegGcm4L3ExHFmATJmXl5VECRIHJPxhfZwxpuYTsf4U'\n",
    "access_token = '2931998159-oEfo3wO1SsEkil6NJ1T3Wni7lvdciTKLIvNeUz3'\n",
    "access_token_secret = 'Pu7kueCRteEwU28vzqpsCh0Y0AQ9y0wIqW8VssrZUoDDN'\n",
    "client_id = 'cl9xZUpDZE9Bb01aZUdIWWQ3aFM6MTpjaQ'\n",
    "client_id_secret = 'qHcKpBGB1YLgIQdRfcgMf4YCBzZpYy_OQlkf67mE_afJ1T2C3l'\n",
    "\n",
    "# Authenticate\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1833964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets(query, max_tweets):\n",
    "    tweets = tweepy.Cursor(api.search_tweets, q=query, lang=\"en\", tweet_mode='extended').items(max_tweets)\n",
    "    \n",
    "    tweet_list = []\n",
    "    for tweet in tweets:\n",
    "        tweet_list.append(tweet.full_text)\n",
    "\n",
    "    return tweet_list\n",
    "\n",
    "product_name = \"ChatGPT\"\n",
    "max_tweets = 50\n",
    "tweets_about_product = search_tweets(product_name, max_tweets)\n",
    "\n",
    "# Print the fetched tweets\n",
    "for tweet in tweets_about_product:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the tweets in a database\n",
    "\n",
    "def create_database():\n",
    "    # Connect to SQLite database (it will be created if it doesn't exist)\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "\n",
    "    # Create a new SQLite table with columns for different tweet attributes\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS tweets\n",
    "                 (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                  tweet_text TEXT,\n",
    "                  query TEXT,\n",
    "                  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    \n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "create_database()\n",
    "\n",
    "def store_tweet(tweet_text, query):\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Insert a new row of data\n",
    "    cur.execute(\"INSERT INTO tweets (tweet_text, query) VALUES (?, ?)\", (tweet_text, query))\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "# For fetching the data in the database later\n",
    "def get_tweets_by_query(query):\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Select tweets that match the query\n",
    "    cur.execute(\"SELECT tweet_text FROM tweets WHERE query=?\", (query,))\n",
    "    all_tweets = cur.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "    return all_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689b19a",
   "metadata": {},
   "source": [
    "# Step 3: Data Processing\n",
    "3.1 Clean and Preprocess Data\n",
    "Implement functions to clean tweets (removing URLs, mentions, hashtags, and special characters).\n",
    "Normalize text data (like converting to lowercase, removing punctuation).\n",
    "\n",
    "Remove URLs: URLs in tweets can be removed as they usually don't contribute to sentiment analysis.\n",
    "Remove Mentions and Hashtags: Mentions (@usernames) and hashtags (#hashtag) can also be removed or kept based on your analysis requirement.\n",
    "Remove Special Characters and Numbers: Special characters and numbers often don't contribute to sentiment analysis and can be removed.\n",
    "Convert to Lowercase: Convert all texts to lowercase to maintain consistency.\n",
    "\n",
    "3.2 Data Storage\n",
    "Decide on how you'll store the fetched tweets (e.g., in a database or files).\n",
    "Implement the storage mechanism in your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f370c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)  # Remove URLs\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)  # Remove mentions\n",
    "    tweet = re.sub(r'#\\S+', '', tweet)  # Remove hashtags\n",
    "    tweet = re.sub(r'[^A-Za-z\\s]', '', tweet)  # Remove special characters and numbers\n",
    "    tweet = tweet.lower()  # Convert to lowercase\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc231ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tweet(tweet):\n",
    "    words = tweet.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    words = tweet.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = clean_tweet(tweet)\n",
    "    tweet = lemmatize_tweet(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4658d0",
   "metadata": {},
   "source": [
    "# Step 4: Sentiment Analysis\n",
    "4.1 Choose Sentiment Analysis Method\n",
    "Decide whether to use a pre-built sentiment analysis tool or to train your own model.\n",
    "If using pre-built tools, test them to see which fits your needs best.\n",
    "\n",
    "4.2 Implement Sentiment Analysis\n",
    "Integrate the sentiment analysis into your data processing pipeline.\n",
    "Test the accuracy and adjust as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d8928",
   "metadata": {},
   "source": [
    "# Step 5: Building the Web Interface\n",
    "5.1 Design the UI\n",
    "Sketch a basic layout of the web interface.\n",
    "Decide on the functionalities (e.g., search input, results display).\n",
    "\n",
    "5.2 Develop the Web Application\n",
    "Use Flask or Django to build the web interface.\n",
    "Implement the front-end to allow users to input a brand or product name.\n",
    "Connect the back-end to handle requests and display results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67238fc9",
   "metadata": {},
   "source": [
    "# Step 6: Visualization and Reporting\n",
    "6.1 Implement Data Visualization\n",
    "Use libraries like Matplotlib or Plotly to create visualizations of the sentiment analysis.\n",
    "Integrate these visualizations into your web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f85c92",
   "metadata": {},
   "source": [
    "# Step 7: Testing and Validation\n",
    "7.1 Testing\n",
    "Conduct thorough testing of your application, covering edge cases.\n",
    "Test the system's performance and accuracy.\n",
    "\n",
    "7.2 User Testing\n",
    "Get feedback from potential users on usability and functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89b676",
   "metadata": {},
   "source": [
    "# Step 8: Deployment and Maintenance\n",
    "8.1 Choose a Deployment Platform\n",
    "Decide where to host your web application (e.g., Heroku, AWS, Google Cloud).\n",
    "\n",
    "8.2 Deploy the Application\n",
    "Deploy your web application.\n",
    "Ensure it's running smoothly and is able to handle multiple users.\n",
    "\n",
    "8.3 Plan for Maintenance\n",
    "Set up a system for regular maintenance and updates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
