{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386c985f",
   "metadata": {},
   "source": [
    "In this project, we are predicting trends in technology adoption and interest based on social media (Twitter) data. Specifically, the model aims to forecast the following:\n",
    "\n",
    "1. **Volume of Discussions**: Predicting the number of tweets or social media posts related to specific technologies, gadgets, or software within a given time frame in the future (e.g., daily, weekly). This serves as an indicator of public interest and awareness levels.\n",
    "\n",
    "2. **Sentiment Trends**: Forecasting the overall sentiment (positive, negative, neutral) associated with these technologies in the social media discourse. This could involve predicting the average sentiment score or the proportion of tweets falling into each sentiment category for upcoming days.\n",
    "\n",
    "3. **Combination of Volume and Sentiment**: A more comprehensive approach might involve predicting both the volume of discussion and the sentiment concurrently. This dual prediction can provide a more nuanced understanding of how public interest and perception might evolve over time.\n",
    "\n",
    "### Example Predictions\n",
    "- **Before a Product Launch**: If there's an upcoming release of a new gadget, the model might predict an increase in the volume of discussion and potentially the sentiment trend leading up to and following the launch.\n",
    "- **Emerging Technology Trends**: For emerging tech like augmented reality, blockchain, or new software platforms, the model could forecast how discussions (both in volume and sentiment) about these technologies will trend in the short-term future.\n",
    "\n",
    "### Purpose of These Predictions\n",
    "- **Market Insight**: These predictions can provide valuable insights for businesses, marketers, and technologists about consumer interest and sentiment trends, aiding in strategic planning and decision-making.\n",
    "- **Product Strategy**: For tech companies, understanding how public interest and sentiment are likely to shift can inform product development, marketing strategies, and customer engagement plans.\n",
    "- **Investment Decisions**: Investors in technology sectors might use these predictions to gauge potential market reactions to new technologies or products.\n",
    "\n",
    "The predictions, therefore, are not just about the raw data but also about interpreting the data to extract meaningful trends and insights that can inform various strategic decisions in the technology domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c50608bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import nltk\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb0fd8",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "Sources: Gather data from social media. We will be using Twitter API to search and get tweets with relevant keywords\n",
    "\n",
    "Keywords: Identify relevant keywords for each technology (e.g., \"artificial intelligence\", \"augmented reality\", \"blockchain\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f7893d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martificial intelligence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maugmented reality\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblockchain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords:\n\u001b[1;32m---> 27\u001b[0m     stream\u001b[38;5;241m.\u001b[39madd_rules(tweepy\u001b[38;5;241m.\u001b[39mStreamRule(keyword))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Start streaming\u001b[39;00m\n\u001b[0;32m     30\u001b[0m stream\u001b[38;5;241m.\u001b[39mfilter()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\streaming.py:333\u001b[0m, in \u001b[0;36mStreamingClient.add_rules\u001b[1;34m(self, add, **params)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m         json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: rule\u001b[38;5;241m.\u001b[39mvalue})\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/2/tweets/search/stream/rules\u001b[39m\u001b[38;5;124m\"\u001b[39m, params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    335\u001b[0m     endpoint_parameters\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdry_run\u001b[39m\u001b[38;5;124m\"\u001b[39m,), json\u001b[38;5;241m=\u001b[39mjson, data_type\u001b[38;5;241m=\u001b[39mStreamRule\n\u001b[0;32m    336\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:129\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, route, params\u001b[38;5;241m=\u001b[39m{}, endpoint_parameters\u001b[38;5;241m=\u001b[39m(), json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m ):\n\u001b[0;32m    127\u001b[0m     request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[1;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(method, route, params\u001b[38;5;241m=\u001b[39mrequest_params,\n\u001b[0;32m    130\u001b[0m                             json\u001b[38;5;241m=\u001b[39mjson, user_auth\u001b[38;5;241m=\u001b[39muser_auth)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_type \u001b[38;5;129;01mis\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:100\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(response)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(response)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFound(response)\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal."
     ]
    }
   ],
   "source": [
    "# Twitter API keys\n",
    "# Consumer Keys\n",
    "api_key = '6zl6Rx7oINQIk3ZjINThyEjqG'\n",
    "api_secret_key = 'TUdgoZYLxwine6O6O33pvosurWAIghU90GyP0sxwxMrOUk60dZ'\n",
    "\n",
    "# Authentication Tokens\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAEPSjQEAAAAADjvwNAQMd146tNbpNMGZgs422m0%3D3zUK6OCejvZ60YvmYHrcxWc5zkHI5ARro9rpkxx2sDzmjM9mFs'\n",
    "access_token = '2931998159-oEfo3wO1SsEkil6NJ1T3Wni7lvdciTKLIvNeUz3'\n",
    "access_token_secret = 'Pu7kueCRteEwU28vzqpsCh0Y0AQ9y0wIqW8VssrZUoDDN'\n",
    "\n",
    "# Client ID and Client Secret\n",
    "client_id = 'cl9xZUpDZE9Bb01aZUdIWWQ3aFM6MTpjaQ'\n",
    "client_id_secret = 'qHcKpBGB1YLgIQdRfcgMf4YCBzZpYy_OQlkf67mE_afJ1T2C3l'\n",
    "\n",
    "# Authenticate\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# Define a class to listen to tweets\n",
    "class MyStream(tweepy.StreamingClient):\n",
    "    def on_tweet(self, tweet):\n",
    "        print(tweet.text)\n",
    "\n",
    "\n",
    "stream = MyStream(bearer_token=bearer_token)\n",
    "\n",
    "# Add rules (keywords) to the stream\n",
    "keywords = [\"artificial intelligence\", \"augmented reality\", \"blockchain\"]\n",
    "for keyword in keywords:\n",
    "    stream.add_rules(tweepy.StreamRule(keyword))\n",
    "\n",
    "# Start streaming\n",
    "stream.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00debcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the tweets in a database\n",
    "\n",
    "def create_database():\n",
    "    # Connect to SQLite database (it will be created if it doesn't exist)\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "\n",
    "    # Create a new SQLite table with columns for different tweet attributes\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS tweets\n",
    "                 (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                  tweet_text TEXT,\n",
    "                  query TEXT,\n",
    "                  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    \n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "create_database()\n",
    "\n",
    "def store_tweet(tweet_text, query):\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Insert a new row of data\n",
    "    cur.execute(\"INSERT INTO tweets (tweet_text, query) VALUES (?, ?)\", (tweet_text, query))\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "# For fetching the data in the database later\n",
    "def get_tweets_by_query(query):\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Select tweets that match the query\n",
    "    cur.execute(\"SELECT tweet_text FROM tweets WHERE query=?\", (query,))\n",
    "    all_tweets = cur.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "    return all_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ecfa3",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "Cleaning: Remove irrelevant content, special characters, and URLs.\n",
    "Normalization: Convert text to a standard format (e.g., lowercase, stemming).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201eceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)  # Remove URLs\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)  # Remove mentions\n",
    "    tweet = re.sub(r'#\\S+', '', tweet)  # Remove hashtags\n",
    "    tweet = re.sub(r'[^A-Za-z\\s]', '', tweet)  # Remove special characters and numbers\n",
    "    tweet = tweet.lower()  # Convert to lowercase\n",
    "    return tweet\n",
    "\n",
    "cleaned_tweets = clean_tweet(tweets_about_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b81d29",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "Sentiment Detection Tool: Use pre-built libraries like NLTK, TextBlob, or build a custom model using machine learning frameworks like TensorFlow or PyTorch.\n",
    "Classification: Classify the sentiment of each piece of text as positive, negative, or neutral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6840c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tweet(tweet):\n",
    "    words = tweet.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    words = tweet.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = clean_tweet(tweet)\n",
    "    tweet = lemmatize_tweet(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c3a6e",
   "metadata": {},
   "source": [
    "## 4. Time Series Analysis\n",
    "Aggregation: Aggregate sentiment scores over time (daily, weekly).\n",
    "Trends Analysis: Use time series analysis techniques to identify trends. Libraries like Pandas and statsmodels can be helpful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb1102",
   "metadata": {},
   "source": [
    "## 5. Forecasting\n",
    "Model Selection: Choose a forecasting model like ARIMA, SARIMA, or LSTM (for deep learning approaches).\n",
    "Prediction: Use the model to predict future trends in sentiment and discussion volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e551cfb",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "Tools: Use libraries like Matplotlib or Plotly to visualize trends and forecasts.\n",
    "Dashboard: Consider building a dashboard using Dash or Streamlit for real-time analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc52002",
   "metadata": {},
   "source": [
    "## 7. Continuous Improvement and Updating\n",
    "Feedback Loop: Incorporate new data regularly to update the models.\n",
    "Model Tuning: Continuously evaluate and tune the models for better accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38a8a2",
   "metadata": {},
   "source": [
    "## 8. Deployment\n",
    "Web Application: Deploy as a web application using frameworks like Flask or Django.\n",
    "APIs: Create APIs for accessing the analysis and forecasts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
