{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4b1c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import tweepy\n",
    "\n",
    "# Data Storage\n",
    "import sqlite3\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# For advanced data manipulation\n",
    "from scipy import stats\n",
    "\n",
    "# For machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# For working with APIs\n",
    "import json\n",
    "\n",
    "# For datetime operations\n",
    "from datetime import datetime\n",
    "\n",
    "# Additional utilities\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "# For time series analysis\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# For real-time data streaming\n",
    "import websocket\n",
    "import socket\n",
    "\n",
    "# For asynchronous programming (useful for real-time data processing)\n",
    "import asyncio\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f37102",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "## 2.1 Twitter API Access\n",
    "Apply for Access: If you haven't already, apply for a Twitter Developer account at developer.twitter.com. You'll need to explain your project's purpose and how you'll use the data.\n",
    "Create an Application: Once approved, create a new application in the Twitter Developer portal to get your API keys and tokens â€” these are necessary for accessing the Twitter API.\n",
    "\n",
    "## 2.2 Understand Twitter API Limitations\n",
    "Rate Limits: Familiarize yourself with the Twitter API rate limits to avoid hitting the cap on the number of requests.\n",
    "Data Availability: Twitter API provides access to tweets from the last 7 days for the standard search API, and more historical data with the premium or enterprise tiers.\n",
    "\n",
    "## 2.3 Develop Data Collection Script\n",
    "Install Tweepy: Use the Python library tweepy for interacting with the Twitter API. Install it via pip (pip install tweepy).\n",
    "Authentication: Use your API keys and tokens to authenticate your requests.\n",
    "Querying Tweets: Write a function to query tweets based on the product or brand name inputted by the user. Use query parameters effectively to filter and retrieve relevant tweets.\n",
    "Handling Rate Limiting: Implement logic to handle rate limiting by the Twitter API, such as waiting and retrying after a certain period.\n",
    "\n",
    "## 2.4 Store Collected Data\n",
    "Temporary Storage: Initially, you might store the tweets in a temporary data structure like a list or a Pandas DataFrame.\n",
    "Database Storage: For long-term storage and retrieval, consider saving the tweets to a database. Choose between SQL or NoSQL based on your preference and the data's structure.\n",
    "\n",
    "## 2.5 Error Handling\n",
    "API Errors: Implement error handling for issues like network errors, API rate limits, or invalid responses.\n",
    "Data Quality Checks: Put checks in place to ensure the quality of the data collected (e.g., filtering out irrelevant or spammy tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3703a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API keys\n",
    "api_key = 'F0qKO41dErn04DpsRuAtnnSaT'\n",
    "api_secret_key = 'TKTT695N6shmAVxsSVzVRGUF9CSKLqoIPrkeHLDHHfj5UaNHUv'\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAEPSjQEAAAAA3DQLgk5ybCdfGUtqI%2FKv4SruAHY%3DhPf2TpZzegGcm4L3ExHFmATJmXl5VECRIHJPxhfZwxpuYTsf4U'\n",
    "access_token = '2931998159-oEfo3wO1SsEkil6NJ1T3Wni7lvdciTKLIvNeUz3'\n",
    "access_token_secret = 'Pu7kueCRteEwU28vzqpsCh0Y0AQ9y0wIqW8VssrZUoDDN'\n",
    "client_id = 'cl9xZUpDZE9Bb01aZUdIWWQ3aFM6MTpjaQ'\n",
    "client_id_secret = 'qHcKpBGB1YLgIQdRfcgMf4YCBzZpYy_OQlkf67mE_afJ1T2C3l'\n",
    "\n",
    "# Authenticate\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc02cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets(query, max_tweets):\n",
    "    tweets = tweepy.Cursor(api.search_tweets, q=query, lang=\"en\", tweet_mode='extended').items(max_tweets)\n",
    "    \n",
    "    tweet_list = []\n",
    "    for tweet in tweets:\n",
    "        tweet_list.append(tweet.full_text)\n",
    "\n",
    "    return tweet_list\n",
    "\n",
    "product_name = \"ChatGPT\"\n",
    "max_tweets = 50\n",
    "tweets_about_product = search_tweets(product_name, max_tweets)\n",
    "\n",
    "# Print the fetched tweets\n",
    "for tweet in tweets_about_product:\n",
    "    print(tweet)\n",
    "    store_tweet(tweet, product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc31486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the tweets in a database\n",
    "\n",
    "def create_database():\n",
    "    # Connect to SQLite database (it will be created if it doesn't exist)\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "\n",
    "    # Create a new SQLite table with columns for different tweet attributes\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS tweets\n",
    "                 (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                  tweet_text TEXT,\n",
    "                  query TEXT,\n",
    "                  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    \n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "create_database()\n",
    "\n",
    "def store_tweet(tweet_text, query):\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Insert a new row of data\n",
    "    cur.execute(\"INSERT INTO tweets (tweet_text, query) VALUES (?, ?)\", (tweet_text, query))\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "# For fetching the data in the database later\n",
    "def get_tweets_by_query(query):\n",
    "    conn = sqlite3.connect('twitter_data.db')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Select tweets that match the query\n",
    "    cur.execute(\"SELECT tweet_text FROM tweets WHERE query=?\", (query,))\n",
    "    all_tweets = cur.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "    return all_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8db364",
   "metadata": {},
   "source": [
    "# Step 3: Data Processing\n",
    "3.1 Clean and Preprocess Data\n",
    "Implement functions to clean tweets (removing URLs, mentions, hashtags, and special characters).\n",
    "Normalize text data (like converting to lowercase, removing punctuation).\n",
    "\n",
    "Remove URLs: URLs in tweets can be removed as they usually don't contribute to sentiment analysis.\n",
    "Remove Mentions and Hashtags: Mentions (@usernames) and hashtags (#hashtag) can also be removed or kept based on your analysis requirement.\n",
    "Remove Special Characters and Numbers: Special characters and numbers often don't contribute to sentiment analysis and can be removed.\n",
    "Convert to Lowercase: Convert all texts to lowercase to maintain consistency.\n",
    "\n",
    "3.2 Data Storage\n",
    "Decide on how you'll store the fetched tweets (e.g., in a database or files).\n",
    "Implement the storage mechanism in your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d6cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)  # Remove URLs\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)  # Remove mentions\n",
    "    tweet = re.sub(r'#\\S+', '', tweet)  # Remove hashtags\n",
    "    tweet = re.sub(r'[^A-Za-z\\s]', '', tweet)  # Remove special characters and numbers\n",
    "    tweet = tweet.lower()  # Convert to lowercase\n",
    "    return tweet\n",
    "\n",
    "cleaned_tweets = clean_tweet(tweets_about_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tweet(tweet):\n",
    "    words = tweet.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    words = tweet.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = clean_tweet(tweet)\n",
    "    tweet = lemmatize_tweet(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b0ead",
   "metadata": {},
   "source": [
    "# Step 4: Data Aggregation for Time Series\n",
    "**Aggregate Data:** Aggregate the data by your chosen time intervals (e.g., daily). Youâ€™ll want to sum up the number of tweets and calculate average sentiment scores for each interval.\n",
    "\n",
    "**Exploratory Data Analysis Trend Analysis:** Visualize the volume of tweets and average sentiment over time to identify patterns, trends, and anomalies. Correlation Analysis: Optionally, check if there's any correlation between the volume of tweets or sentiment with external events or announcements in the tech world.\n",
    "\n",
    "**Time-Series Forecasting Model Selection:** Choose a suitable model for time-series forecasting. ARIMA, SARIMA, and LSTM neural networks are common choices. Feature Engineering: Include relevant time-based features and any other features that might improve the model. Model Training: Train your model on the historical data.\n",
    "\n",
    "**Model Evaluation Validation:** Validate your model on a separate test set. Performance Metrics: Use metrics like MAE, RMSE, or others relevant for time-series to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(tweets_about_product)\n",
    "\n",
    "# Convert timestamp to datetime and set as index\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Resample and aggregate\n",
    "aggregated_df = df.resample('D').agg({'text': 'count', 'sentiment': 'mean'}) # 'D' for daily\n",
    "aggregated_df.rename(columns={'text': 'tweet_count'}, inplace=True)\n",
    "\n",
    "# Now, aggregated_df contains daily tweet counts and average sentiment\n",
    "print(aggregated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values with the previous day's data (forward fill)\n",
    "# It contains columns like 'tweet_count' and 'average_sentiment'\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values before forward fill:\")\n",
    "print(aggregated_df.isnull().sum())\n",
    "\n",
    "# Apply forward fill\n",
    "aggregated_df.ffill(inplace=True)\n",
    "\n",
    "# Check to ensure missing values are filled\n",
    "print(\"Missing values after forward fill:\")\n",
    "print(aggregated_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50977cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ARIMA (Example)\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Assuming df is your DataFrame and 'tweet_count' is the column\n",
    "# Check for stationarity\n",
    "result = adfuller(df['tweet_count'])\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "\n",
    "# Fit ARIMA model (example parameters)\n",
    "model = ARIMA(df['tweet_count'], order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Vector Autoregression (Example)\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Assuming df is your DataFrame with multiple columns like 'tweet_count' and 'sentiment'\n",
    "model = VAR(df)\n",
    "model_fit = model.fit(maxlags=15, ic='aic')\n",
    "print(model_fit.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc5ffe",
   "metadata": {},
   "source": [
    "# Step 5: Exploratory Data Analysis\n",
    "**Trend Analysis:** Visualize the volume of tweets and average sentiment over time to identify patterns, trends, and anomalies.\n",
    "\n",
    "**Correlation Analysis:** Optionally, check if there's any correlation between the volume of tweets or sentiment with external events or announcements in the tech world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5edde00",
   "metadata": {},
   "source": [
    "# Step 6: Time-Series Forecasting\n",
    "**Model Selection:** Choose a suitable model for time-series forecasting. ARIMA, SARIMA, and LSTM neural networks are common choices.\n",
    "\n",
    "**Feature Engineering:** Include relevant time-based features and any other features that might improve the model.\n",
    "Model Training: Train your model on the historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17fc02",
   "metadata": {},
   "source": [
    "# Step 7: Model Evaluation\n",
    "**Validation:** Validate your model on a separate test set.\n",
    "\n",
    "**Performance Metrics:** Use metrics like MAE, RMSE, or others relevant for time-series to evaluate the model's performance.# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d660d",
   "metadata": {},
   "source": [
    "# Step 8: Prediction and Visualization\n",
    "**Make Predictions:** Use the model to make predictions about future trends in discussions on emerging technologies.\n",
    "\n",
    "**Visualization:** Create visualizations to represent the predicted trends in an understandable and insightful way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
