{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386c985f",
   "metadata": {},
   "source": [
    "In this project, we are predicting trends in technology adoption and interest based on social media (Twitter) data. Specifically, the model aims to forecast the following:\n",
    "\n",
    "1. **Volume of Discussions**: Predicting the number of tweets or social media posts related to specific technologies, gadgets, or software within a given time frame in the future (e.g., daily, weekly). This serves as an indicator of public interest and awareness levels.\n",
    "\n",
    "2. **Sentiment Trends**: Forecasting the overall sentiment (positive, negative, neutral) associated with these technologies in the social media discourse. This could involve predicting the average sentiment score or the proportion of tweets falling into each sentiment category for upcoming days.\n",
    "\n",
    "3. **Combination of Volume and Sentiment**: A more comprehensive approach might involve predicting both the volume of discussion and the sentiment concurrently. This dual prediction can provide a more nuanced understanding of how public interest and perception might evolve over time.\n",
    "\n",
    "### Example Predictions\n",
    "- **Before a Product Launch**: If there's an upcoming release of a new gadget, the model might predict an increase in the volume of discussion and potentially the sentiment trend leading up to and following the launch.\n",
    "- **Emerging Technology Trends**: For emerging tech like augmented reality, blockchain, or new software platforms, the model could forecast how discussions (both in volume and sentiment) about these technologies will trend in the short-term future.\n",
    "\n",
    "### Purpose of These Predictions\n",
    "- **Market Insight**: These predictions can provide valuable insights for businesses, marketers, and technologists about consumer interest and sentiment trends, aiding in strategic planning and decision-making.\n",
    "- **Product Strategy**: For tech companies, understanding how public interest and sentiment are likely to shift can inform product development, marketing strategies, and customer engagement plans.\n",
    "- **Investment Decisions**: Investors in technology sectors might use these predictions to gauge potential market reactions to new technologies or products.\n",
    "\n",
    "The predictions, therefore, are not just about the raw data but also about interpreting the data to extract meaningful trends and insights that can inform various strategic decisions in the technology domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c50608bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import nltk\n",
    "import sqlite3\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d15e9",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "Sources: Gather data from social media. We will be using Twitter API to search and get tweets with relevant keywords\n",
    "\n",
    "Keywords: Identify relevant keywords for each technology (e.g., \"artificial intelligence\", \"augmented reality\", \"blockchain\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f7893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API keys\n",
    "\n",
    "# Consumer Keys\n",
    "# MSML apis\n",
    "api_key = 'fQyQfxNjgLk8NDoVt339h8K0g'\n",
    "api_secret_key = '5wHUc4mrkVn1R9pR7tVaNXkKuB6Le1qIpSqKA3nb9H70rEVqiz'\n",
    "\n",
    "# Authentication Tokens\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAJu0rQEAAAAAisjoiU156fEdDphPqie4eiNi0L0%3DByO7mVMXobvQB1IA1XCLUWWTLbFtjSu9TcBh06OpFJSRldwkHa'\n",
    "\n",
    "access_token = '2931998159-ngeYrsqwmVvs1jYjpZcCFBzO2xm0j2wsqokBLK6'\n",
    "access_token_secret = 'CGo43zg5cX2KDdyACKDVIUtrULMV1SCBjPVNogCW1UKKs'\n",
    "\n",
    "# Authenticate\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n",
    "# Define the start and end time for the query (5 months ago to now)\n",
    "start_time = datetime.utcnow() - timedelta(days=150)  # Approximately 5 months\n",
    "start_time = start_time.strftime('%Y-%m-%dT%H:%M:%SZ')  # Format as required\n",
    "end_time = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fdcd92a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Getting the tweets from twitter\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124martificial intelligence -is:retweet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m tweets \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39msearch_all_tweets(query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[0;32m      4\u001b[0m                                   start_time\u001b[38;5;241m=\u001b[39mstart_time,\n\u001b[0;32m      5\u001b[0m                                   end_time\u001b[38;5;241m=\u001b[39mend_time,\n\u001b[0;32m      6\u001b[0m                                   tweet_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_annotations\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      7\u001b[0m                                   max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Extract data from the response\u001b[39;00m\n\u001b[0;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m: tweet\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: tweet\u001b[38;5;241m.\u001b[39mcreated_at} \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets\u001b[38;5;241m.\u001b[39mdata]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:1163\u001b[0m, in \u001b[0;36mClient.search_all_tweets\u001b[1;34m(self, query, **params)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"search_all_tweets( \\\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;124;03m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;124;03m.. _pagination: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/paginate\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m-> 1163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/2/tweets/search/all\u001b[39m\u001b[38;5;124m\"\u001b[39m, params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m   1165\u001b[0m     endpoint_parameters\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpansions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_results\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedia.fields\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplace.fields\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoll.fields\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msince_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msort_order\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet.fields\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muntil_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser.fields\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1170\u001b[0m     ), data_type\u001b[38;5;241m=\u001b[39mTweet\n\u001b[0;32m   1171\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:129\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, route, params\u001b[38;5;241m=\u001b[39m{}, endpoint_parameters\u001b[38;5;241m=\u001b[39m(), json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m ):\n\u001b[0;32m    127\u001b[0m     request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[1;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(method, route, params\u001b[38;5;241m=\u001b[39mrequest_params,\n\u001b[0;32m    130\u001b[0m                             json\u001b[38;5;241m=\u001b[39mjson, user_auth\u001b[38;5;241m=\u001b[39muser_auth)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_type \u001b[38;5;129;01mis\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tweepy\\client.py:100\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(response)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(response)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFound(response)\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal."
     ]
    }
   ],
   "source": [
    "# Getting the tweets from twitter\n",
    "query = 'artificial intelligence -is:retweet'\n",
    "tweets = client.search_all_tweets(query=query,\n",
    "                                  start_time=start_time,\n",
    "                                  end_time=end_time,\n",
    "                                  tweet_fields=['context_annotations', 'created_at'],\n",
    "                                  max_results=100)\n",
    "\n",
    "# Extract data from the response\n",
    "data = [{'Tweet': tweet.text, 'Timestamp': tweet.created_at} for tweet in tweets.data]\n",
    "\n",
    "# Create a DataFrame\n",
    "tweets_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tweets_to_database(tweets_df):\n",
    "    # Create a SQLite database connection\n",
    "    conn = sqlite3.connect('tweets_database.db')\n",
    "\n",
    "    # Write the DataFrame to a SQLite table\n",
    "    tweets_df.to_sql('tweets', conn, if_exists='replace', index=False)\n",
    "\n",
    "    # Optionally, read the table back from the database to verify\n",
    "    tweets_df_from_sql = pd.read_sql('SELECT * FROM tweets', conn)\n",
    "\n",
    "    # Display the DataFrame read from the database\n",
    "    print(tweets_df_from_sql)\n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "    \n",
    "add_tweets_to_database(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00debcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fetching the data in the database later\n",
    "def get_tweets_by_query():\n",
    "    conn = sqlite3.connect('tweets_database.db')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Select tweets that match the query\n",
    "    cur.execute(\"SELECT Tweet FROM tweets\")\n",
    "    all_tweets = cur.fetchall()\n",
    "    \n",
    "    print(all_tweets)\n",
    "\n",
    "    conn.close()\n",
    "    return all_tweets\n",
    "\n",
    "get_tweets_by_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d96df",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "Cleaning: Remove irrelevant content, special characters, and URLs.\n",
    "Normalization: Convert text to a standard format (e.g., lowercase, stemming).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201eceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tweet cleaning function\n",
    "def clean_tweet(tweet):\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove @usernames and #hashtags\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Assuming tweets_df is your DataFrame and 'Tweet' is the column with tweet texts\n",
    "# Apply the cleaning function to each tweet\n",
    "tweets_df['Cleaned_Tweet'] = tweets_df['Tweet'].apply(clean_tweet)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c924c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'ContextAnnotations' column\n",
    "tweets_df = tweets_df.drop('Tweet', axis=1)\n",
    "\n",
    "# Display the DataFrame to confirm the column is dropped\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0517e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the tweet does not contain the keywords then remove it\n",
    "\n",
    "# query words\n",
    "query_words = ['Artificial Intelligence', 'ai']\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = tweets_df['Cleaned_Tweet'].str.contains('|'.join(query_words), case=False, na=False)\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_tweets_df = tweets_df[mask]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf97e00",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "Sentiment Detection Tool: Use pre-built libraries like TextBlob.\n",
    "Classification: Classify the sentiment of each piece of text as positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6840c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply sentiment analysis\n",
    "def analyze_sentiment(tweet):\n",
    "    analysis = TextBlob(tweet)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'positive', polarity\n",
    "    elif polarity == 0:\n",
    "        return 'neutral', polarity\n",
    "    else:\n",
    "        return 'negative', polarity\n",
    "\n",
    "# Apply the function to each tweet\n",
    "tweets_df['Sentiment'], tweets_df['Polarity'] = zip(*tweets_df['Cleaned_Tweet'].apply(analyze_sentiment))\n",
    "\n",
    "# Display the first few rows of the DataFrame with sentiment data\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23f9a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure your DataFrame has 'Sentiment' column from the sentiment analysis step\n",
    "# Sentiment Distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Sentiment', data=tweets_df)\n",
    "plt.title('Sentiment Distribution in Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Over Time (assuming there's a 'Timestamp' column)\n",
    "# Convert 'Timestamp' to datetime if not already\n",
    "tweets_df['Timestamp'] = pd.to_datetime(tweets_df['Timestamp'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "tweets_df.resample('D', on='Timestamp')['Sentiment'].value_counts().unstack().plot(kind='line')\n",
    "plt.title('Sentiment Over Time')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.xlabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed798492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud (for positive sentiment tweets as an example)\n",
    "positive_tweets = ' '.join(tweets_df[tweets_df['Sentiment'] == 'positive']['Cleaned_Tweet'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color ='white').generate(positive_tweets)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Positive Sentiment Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tweets_df['Sentiment'].dtype == 'object':\n",
    "    tweets_df['Sentiment'] = tweets_df['Sentiment'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e7d97",
   "metadata": {},
   "source": [
    "## 4. Time Series Analysis\n",
    "Aggregation: Aggregate sentiment scores over time (daily, weekly).\n",
    "Trends Analysis: Use time series analysis techniques to identify trends. Libraries like Pandas and statsmodels can be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36983fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Timestamp' to datetime and set as index\n",
    "tweets_df['Timestamp'] = pd.to_datetime(tweets_df['Timestamp'])\n",
    "tweets_df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Resample and aggregate sentiment scores\n",
    "# For example, calculating daily mean sentiment\n",
    "daily_sentiment = tweets_df['Polarity'].resample('D').mean()\n",
    "\n",
    "# Plot daily sentiment\n",
    "plt.figure(figsize=(12, 6))\n",
    "daily_sentiment.plot()\n",
    "plt.title('Daily Average Sentiment Polarity')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Polarity')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Augmented Dickey-Fuller test for stationarity\n",
    "adf_test = adfuller(daily_sentiment.dropna())\n",
    "print('ADF Statistic:', adf_test[0])\n",
    "print('p-value:', adf_test[1])\n",
    "\n",
    "# Interpret the results (using a typical p-value threshold of 0.05)\n",
    "if adf_test[1] < 0.05:\n",
    "    print(\"The time series is likely stationary.\")\n",
    "else:\n",
    "    print(\"The time series is likely non-stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240598ce",
   "metadata": {},
   "source": [
    "## 5. Forecasting\n",
    "Model Selection: Choose a forecasting model like ARIMA, SARIMA, or LSTM (for deep learning approaches).\n",
    "Prediction: Use the model to predict future trends in sentiment and discussion volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803236e5",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "Tools: Use libraries like Matplotlib or Plotly to visualize trends and forecasts.\n",
    "Dashboard: Consider building a dashboard using Dash or Streamlit for real-time analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29acf1",
   "metadata": {},
   "source": [
    "## 7. Continuous Improvement and Updating\n",
    "Feedback Loop: Incorporate new data regularly to update the models.\n",
    "Model Tuning: Continuously evaluate and tune the models for better accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c740fe",
   "metadata": {},
   "source": [
    "## 8. Deployment\n",
    "Web Application: Deploy as a web application using frameworks like Flask or Django.\n",
    "APIs: Create APIs for accessing the analysis and forecasts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
